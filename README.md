ğŸ“Œ Overview

This project implements a modular AI Lead Response Assistant that reads a customer property-related enquiry and generates a structured, safe, and human-like response.

The system is designed with a strong focus on:

Structured reasoning

Reliability

Hallucination control

Guardrails against false claims

Modular AI workflow design

The solution uses an open-source LLM (Llama 3.1 8B Instruct via Ollama) running locally.

ğŸ¯ Objective

Given a customer query such as:

â€œI am getting damp patches on my bedroom wall after heavy rain.â€

The system:

Understands the issue category

Extracts structured information

Detects missing details

Generates targeted clarifying questions

Provides safe next steps

Validates output for risky language

Composes a final professional response

ğŸ— System Architecture

The assistant is built as a multi-step modular workflow, not a single LLM call.

User Query
   â†“
Intent Classification
   â†“
Structured Information Extraction (JSON)
   â†“
Missing Information Detection (Deterministic)
   â†“
Clarifying Question Generation
   â†“
Safe Advice Generation
   â†“
Validation Guard (Rule-based)
   â†“
Deterministic Final Response Composition

ğŸ” Design Philosophy
1ï¸âƒ£ Structured Before Generative

Instead of directly generating a reply, the system first extracts structured fields:

{
  "location": "",
  "trigger": "",
  "symptoms": "",
  "duration": "",
  "urgency": ""
}


This enables:

Controlled reasoning

Missing data detection

Targeted clarification

Reduced hallucination risk

2ï¸âƒ£ Deterministic + LLM Hybrid Design

The system combines:

LLM-based reasoning (classification, extraction, generation)

Deterministic Python logic (missing field detection, validation, composition)

This improves reliability and control.

3ï¸âƒ£ Guardrails & Safety

A validation layer checks for:

Absolute claims (e.g., â€œdefinitelyâ€, â€œmust beâ€)

Diagnostic statements

Guarantees or promises

Risky certainty language

This ensures the assistant avoids hallucinated or misleading advice.

ğŸ›  Technologies Used

Llama 3.1 8B Instruct (quantized)

Ollama (local model serving)

Python

Requests library

Model used:

llama3.1:8b-instruct-q4_0


The quantized model ensures compatibility with limited GPU resources.

ğŸ“‚ Project Structure
lead_response_assistant/
â”‚
â”œâ”€â”€ main.py           # Orchestrates the workflow
â”œâ”€â”€ llm_engine.py     # Handles model API calls
â”œâ”€â”€ prompts.py        # Stores structured prompt templates
â”œâ”€â”€ validator.py      # Validation & guardrail logic
â””â”€â”€ README.md

â–¶ How to Run
1ï¸âƒ£ Install Ollama

https://ollama.com

2ï¸âƒ£ Pull Model
ollama pull llama3.1:8b-instruct-q4_0

3ï¸âƒ£ Activate Virtual Environment
python -m venv venv
venv\Scripts\activate
pip install requests

4ï¸âƒ£ Run
python main.py


Enter a customer query when prompted.

âœ… Example Output

Input:

I am getting damp patches on my bedroom wall after heavy rain for the past two weeks.

Output:

Professional acknowledgement

Targeted clarification questions

Safe homeowner guidance

No diagnosis

No promises

Human tone

ğŸ”’ Reliability Features

Strict JSON extraction format

Explicit â€œNot Availableâ€ handling

Deterministic missing field detection

Targeted clarifying questions

Guardrail validation layer

Deterministic final response composition

âš  Limitations

Open-source model may require prompt refinement for edge cases

No conversation memory (single-turn assistant)

No confidence scoring or retry logic (can be added)

No frontend interface (CLI-based interaction)

ğŸš€ Possible Improvements

With more time, I would:

Add automatic retry if JSON parsing fails

Add confidence scoring for extracted fields

Add multi-turn conversation memory

Add evaluation test suite

Deploy via FastAPI endpoint

Add logging system for production monitoring

ğŸ¥ Demonstration

A 3â€“5 minute Loom video explains:

Architecture decisions

Reliability design

Guardrails

Limitations

Future improvements

ğŸ Conclusion

This solution demonstrates:

Structured AI workflow design

Reliability-focused architecture

Hallucination mitigation strategies

Hybrid deterministic + LLM reasoning

Production-oriented modular system thinking
